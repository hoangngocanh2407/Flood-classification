{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":80345,"databundleVersionId":8601308,"sourceType":"competition"}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-04T12:09:01.189198Z","iopub.execute_input":"2024-06-04T12:09:01.189532Z","iopub.status.idle":"2024-06-04T12:09:10.318507Z","shell.execute_reply.started":"2024-06-04T12:09:01.189504Z","shell.execute_reply":"2024-06-04T12:09:10.317546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:09:49.734346Z","iopub.execute_input":"2024-06-04T12:09:49.735089Z","iopub.status.idle":"2024-06-04T12:10:00.101114Z","shell.execute_reply.started":"2024-06-04T12:09:49.735059Z","shell.execute_reply":"2024-06-04T12:10:00.100312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U tensorflow==2.15.0\nimport tensorflow as tf\ntf.__version__","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow==2.15.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade tensorflow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install --upgrade tf-models-official","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"folder_path = \"/kaggle/input/2024-sum-dpl-302-m/devset_images/devset_images\"\nlabel_path = \"/kaggle/input/2024-sum-dpl-302-m/devset_images_gt.csv\"\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:10:05.172175Z","iopub.execute_input":"2024-06-04T12:10:05.173038Z","iopub.status.idle":"2024-06-04T12:10:05.177378Z","shell.execute_reply.started":"2024-06-04T12:10:05.173005Z","shell.execute_reply":"2024-06-04T12:10:05.176261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd \nimport cv2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-06-04T02:51:27.412894Z","iopub.execute_input":"2024-06-04T02:51:27.413295Z","iopub.status.idle":"2024-06-04T02:51:28.607341Z","shell.execute_reply.started":"2024-06-04T02:51:27.413267Z","shell.execute_reply":"2024-06-04T02:51:28.606428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -U scikit-learn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndata = pd.read_csv(label_path) \n# Preview the first 5 lines of the loaded data \ndata_array=data.values\ndata_array[0:10]","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:10:08.931505Z","iopub.execute_input":"2024-06-04T12:10:08.931875Z","iopub.status.idle":"2024-06-04T12:10:08.952132Z","shell.execute_reply.started":"2024-06-04T12:10:08.931844Z","shell.execute_reply":"2024-06-04T12:10:08.951086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=[]\nY_train=[]\nk=0\nfor i in data_array:\n    image = cv2.imread(folder_path+'/'+str(i[0])+'.jpg')\n    if image is not None:\n        #print(k)\n        k=k+1\n        image = cv2.resize(image, (224,224))\n        # print(image.shape)\n        X_train.append(image)\n        Y_train.append(i[1])\nprint(k)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:10:15.540497Z","iopub.execute_input":"2024-06-04T12:10:15.541404Z","iopub.status.idle":"2024-06-04T12:11:16.811705Z","shell.execute_reply.started":"2024-06-04T12:10:15.541365Z","shell.execute_reply":"2024-06-04T12:11:16.810737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(16,10))\nfor i in range(1,11):\n    plt.subplot(2,5,i)\n    plt.grid(False)\n    plt.imshow(X_train[i])\n    plt.xlabel(\"Label \" + str(Y_train[i]))\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=np.array(X_train)\nY_train= np.array(Y_train)\nprint(X_train.shape)\nprint(Y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:11:32.504531Z","iopub.execute_input":"2024-06-04T12:11:32.504897Z","iopub.status.idle":"2024-06-04T12:11:32.750185Z","shell.execute_reply.started":"2024-06-04T12:11:32.504870Z","shell.execute_reply":"2024-06-04T12:11:32.749112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:11:35.156819Z","iopub.execute_input":"2024-06-04T12:11:35.157652Z","iopub.status.idle":"2024-06-04T12:11:35.346286Z","shell.execute_reply.started":"2024-06-04T12:11:35.157620Z","shell.execute_reply":"2024-06-04T12:11:35.345270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB7\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\n\nimg_augmentation = Sequential(\n    [\n        layers.RandomRotation(factor=0.15),\n        layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n        layers.RandomFlip(),\n        layers.RandomContrast(factor=0.1),\n    ],\n    name=\"img_augmentation\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:11:38.691922Z","iopub.execute_input":"2024-06-04T12:11:38.692270Z","iopub.status.idle":"2024-06-04T12:11:39.393651Z","shell.execute_reply.started":"2024-06-04T12:11:38.692245Z","shell.execute_reply":"2024-06-04T12:11:39.392677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport tensorflow as tf\n\n# Giả định rằng X_train và Y_train đã được định nghĩa trước đó\n# và X_train chứa các ảnh có giá trị trong khoảng [0, 1] hoặc [0, 255]\n\nplt.figure(figsize=(16, 10))\nfor i in range(1, 11):\n    plt.subplot(2, 5, i)\n    plt.grid(False)\n    \n    # Lấy một ảnh từ X_train và thêm một chiều batch\n    image = tf.expand_dims(X_train[i], 0)\n    \n    # Thực hiện augmentation trên ảnh\n    augmented_image = img_augmentation(image)\n    \n    # Loại bỏ chiều batch\n    augmented_image = tf.squeeze(augmented_image, axis=0)\n    \n    # Chuyển đổi ảnh về dạng numpy array\n    augmented_image = augmented_image.numpy()\n    \n    # Nếu ảnh đã được chuẩn hóa về khoảng [0, 1], chuyển đổi lại về [0, 255]\n    if augmented_image.max() <= 1.0:\n        augmented_image = augmented_image * 255.0\n    \n    # Đảm bảo ảnh có kiểu dữ liệu uint8\n    augmented_image = augmented_image.astype('uint8')\n    \n    # Hiển thị ảnh\n    plt.imshow(augmented_image)\n    plt.xlabel(\"Label \" + str(Y_train[i]))\n    \nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:11:46.481739Z","iopub.execute_input":"2024-06-04T12:11:46.482117Z","iopub.status.idle":"2024-06-04T12:11:46.492155Z","shell.execute_reply.started":"2024-06-04T12:11:46.482090Z","shell.execute_reply":"2024-06-04T12:11:46.490868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 224\nimport tensorflow as tf\nfrom tensorflow.keras.applications import EfficientNetB4,ResNet101V2\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n#from tensorflow.keras.applications.resnet_v2 import preprocess_input\ndef build_model(num_classes=1):\n    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n    x = preprocess_input(inputs)\n    x = img_augmentation(x)\n    model = EfficientNetB4(include_top=False, input_tensor=x, weights=\"imagenet\")\n\n    # Freeze the pretrained weights\n    model.trainable = False\n\n    # Rebuild top\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dense(1024, activation=\"relu\", name=\"relu\")(x)\n\n    top_dropout_rate = 0.2\n\n    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n\n    x = layers.Dense(128, activation=\"relu\", name=\"relu_2\")(x)\n    #change \n    x = layers.Dropout(0.5, name=\"top_dropout_2\")(x)\n\n    outputs = layers.Dense(num_classes, activation=\"sigmoid\", name=\"pred\")(x)\n\n    # Compile\n    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n    model.compile(\n        optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy',f1]\n    )\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:11:51.910791Z","iopub.execute_input":"2024-06-04T12:11:51.911614Z","iopub.status.idle":"2024-06-04T12:11:51.922859Z","shell.execute_reply.started":"2024-06-04T12:11:51.911586Z","shell.execute_reply":"2024-06-04T12:11:51.921942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = build_model(num_classes=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:11:55.046702Z","iopub.execute_input":"2024-06-04T12:11:55.047111Z","iopub.status.idle":"2024-06-04T12:11:58.418217Z","shell.execute_reply.started":"2024-06-04T12:11:55.047081Z","shell.execute_reply":"2024-06-04T12:11:58.417450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\ndef outer_product(x):\n    #Einstein Notation  [batch,1,1,depth] x [batch,1,1,depth] -> [batch,depth,depth]\n    phi_I = tf.einsum('ijkm,ijkn->imn',x[0],x[1])\n    # Reshape from [batch_size,depth,depth] to [batch_size, depth*depth]\n    phi_I = tf.reshape(phi_I,[-1,x[0].shape[3]*x[1].shape[3]])\n    \n    # Divide by feature map size [sizexsize]\n    size1 = int(x[1].shape[1])\n    size2 = int(x[1].shape[2])\n    phi_I = tf.divide(phi_I, size1*size2)\n    \n    # Take signed square root of phi_I\n    y_ssqrt = tf.multiply(tf.sign(phi_I),tf.sqrt(tf.abs(phi_I)+1e-12))\n    \n    # Apply l2 normalization\n    z_l2 = tf.nn.l2_normalize(y_ssqrt, axis=1)\n    return z_l2","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:12:01.449433Z","iopub.execute_input":"2024-06-04T12:12:01.449783Z","iopub.status.idle":"2024-06-04T12:12:01.457193Z","shell.execute_reply.started":"2024-06-04T12:12:01.449756Z","shell.execute_reply":"2024-06-04T12:12:01.456214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom tensorflow.keras.layers import Convolution2D,Activation,GlobalAveragePooling2D,MaxPooling2D,Flatten,Dense,Dropout,Input,Reshape,Lambda\n\n\ninputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\nx = preprocess_input(inputs)\nbandau = img_augmentation(x)\n# Model 1 \n\nmodel1 = EfficientNetB4(include_top=False, input_tensor=bandau, weights=\"imagenet\")\n\n# Model 2\nmodel2 = EfficientNetB4(include_top=False, input_tensor=bandau, weights=\"imagenet\")\n\nfor i, layer in enumerate(model1.layers):\n    layer._name = 'model1_' + layer.name\n    layer.trainable = False #Freeze all layers\nfor i, layer in enumerate(model2.layers):\n    layer._name = 'model2_' + layer.name\n    layer.trainable = False #Freeze all layers\n\n\n# Rebuild top\n#x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model1.output)\nlast_output1 = layers.BatchNormalization()(model1.output)\n\n# Rebuild top\n#x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model2.output)\nlast_output2 = layers.BatchNormalization()(model2.output)\n\n##\nmodel1_ = Model(inputs=model1.input, outputs=last_output1)\nmodel2_ = Model(inputs=model2.input, outputs=last_output2)\n\nd1=model1_.output\nd2=model2_.output\n\nbilinear = Lambda(outer_product)([d1,d2])\n\nx = Dense(32, activation='relu', name='dense1')(bilinear)\n\nx = layers.Dropout(0.2, name=\"top_dropout\")(x)\n\npredictions=Dense(1, activation='sigmoid', name='predictions')(x)\n\nmodel = Model(inputs=model1.input, outputs=predictions)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:12:07.169052Z","iopub.execute_input":"2024-06-04T12:12:07.169423Z","iopub.status.idle":"2024-06-04T12:12:12.924024Z","shell.execute_reply.started":"2024-06-04T12:12:07.169386Z","shell.execute_reply":"2024-06-04T12:12:12.922588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\nmodel.compile(\n    optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy',f1]\n)\n\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:13:04.965127Z","iopub.execute_input":"2024-06-04T12:13:04.966184Z","iopub.status.idle":"2024-06-04T12:13:05.623357Z","shell.execute_reply.started":"2024-06-04T12:13:04.966139Z","shell.execute_reply":"2024-06-04T12:13:05.622178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath='./dgt301-{epoch:03d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.weights.h5', \n    monitor='val_loss',  \n    save_weights_only=True, \n    save_freq='epoch', \n    verbose=1,\n    save_best_only=True,\n    mode='auto'  # This can be 'auto', 'min', or 'max'\n)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.0,\n    patience=10,\n    verbose=1\n)\n\nreduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=6,\n    verbose=1,\n    min_delta=0.001,  # `epsilon` has been replaced with `min_delta`\n    cooldown=0,\n    min_lr=0.00001\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:13:36.451714Z","iopub.execute_input":"2024-06-04T12:13:36.452465Z","iopub.status.idle":"2024-06-04T12:13:36.458983Z","shell.execute_reply.started":"2024-06-04T12:13:36.452435Z","shell.execute_reply":"2024-06-04T12:13:36.457856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import backend as K\n\n# Precision calculation\ndef precision(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n    predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\n# Recall calculation\ndef recall(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n    possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\n# F1 score calculation\ndef f1(y_true, y_pred):\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    return 2 * ((p * r) / (p + r + K.epsilon()))\n\n# Compile the model with custom metrics\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[f1])\n\n# Callbacks\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath='./dgt301-{epoch:03d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.weights.h5', \n    monitor='val_loss',  \n    save_weights_only=True, \n    save_freq='epoch', \n    verbose=1,\n    save_best_only=True,\n    mode='auto'\n)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.0,\n    patience=10,\n    verbose=1\n)\n\nreduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=6,\n    verbose=1,\n    min_delta=0.001,\n    cooldown=0,\n    min_lr=0.00001\n)\n\n# Fit the model\nhistory = model.fit(\n    x_train,\n    y_train,\n    batch_size=32,\n    epochs=50,\n    validation_data=(x_test, y_test),\n    callbacks=[model_checkpoint, early_stopping, reduce_learning_rate]\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:12:21.622869Z","iopub.execute_input":"2024-06-04T14:12:21.623842Z","iopub.status.idle":"2024-06-04T14:25:27.030784Z","shell.execute_reply.started":"2024-06-04T14:12:21.623797Z","shell.execute_reply":"2024-06-04T14:25:27.029867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('./dgt301-003_loss-0.6633_val_loss-0.6514.weights.h5')\n\ndef unfreeze_model(model):\n    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n    for layer in model.layers[-400:]:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(), metrics=[\"accuracy\",f1]\n    )\n\nunfreeze_model(model)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:30:21.574037Z","iopub.execute_input":"2024-06-04T14:30:21.574708Z","iopub.status.idle":"2024-06-04T14:30:22.842516Z","shell.execute_reply.started":"2024-06-04T14:30:21.574675Z","shell.execute_reply":"2024-06-04T14:30:22.841705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_checkpoint = tf.keras.callbacks.ModelCheckpoint(mode='auto', filepath='./dgt301_turn2-{epoch:03d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.weights.h5', \n                     monitor='val_loss',  \n                     save_weights_only='True', \n                     period=1,\n                     verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                               min_delta=0.0,\n                               patience=10,\n                               verbose=1)\nreduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                         factor=0.2,\n                                         patience=6,\n                                         verbose=1,\n                                         epsilon=0.001,\n                                         cooldown=0,\n                                         min_lr=0.00001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    mode='auto',\n    filepath='./dgt301_turn2-{epoch:03d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.weights.h5', \n    monitor='val_loss',  \n    save_weights_only=True,  # Corrected to boolean True\n    save_freq='epoch',  # Save weights at the end of each epoch\n    verbose=1\n)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.0,\n    patience=10,\n    verbose=1\n)\n\nreduce_learning_rate = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.2,\n    patience=6,\n    verbose=1,\n    min_delta=0.001,\n    cooldown=0,\n    min_lr=0.00001\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:30:32.431420Z","iopub.execute_input":"2024-06-04T14:30:32.432094Z","iopub.status.idle":"2024-06-04T14:30:32.438575Z","shell.execute_reply.started":"2024-06-04T14:30:32.432063Z","shell.execute_reply":"2024-06-04T14:30:32.437605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history2 = model.fit(\n    x_train,\n    y_train,\n    batch_size=32,\n    epochs=50,\n    validation_data=(x_test, y_test),\n    callbacks=[model_checkpoint,early_stopping,reduce_learning_rate]\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T14:30:35.677843Z","iopub.execute_input":"2024-06-04T14:30:35.678637Z","iopub.status.idle":"2024-06-04T14:42:22.631460Z","shell.execute_reply.started":"2024-06-04T14:30:35.678607Z","shell.execute_reply":"2024-06-04T14:42:22.630524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_file = \"/kaggle/input/2024-sum-dpl-302-m/test.csv\"\nfolder_path = \"/kaggle/input/2024-sum-dpl-302-m/testset_images/testset_images\"","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:40:36.689596Z","iopub.execute_input":"2024-06-04T12:40:36.690297Z","iopub.status.idle":"2024-06-04T12:40:36.694472Z","shell.execute_reply.started":"2024-06-04T12:40:36.690264Z","shell.execute_reply":"2024-06-04T12:40:36.693509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(label_file) \ndata_array=data.values","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:40:38.269467Z","iopub.execute_input":"2024-06-04T12:40:38.269792Z","iopub.status.idle":"2024-06-04T12:40:38.291406Z","shell.execute_reply.started":"2024-06-04T12:40:38.269767Z","shell.execute_reply":"2024-06-04T12:40:38.290614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=[]\nk=0\nfor i in data_array:\n  image = cv2.imread(folder_path+'/'+str(i[0])+'.jpg')\n  if image is not None:\n    k=k+1\n    image = cv2.resize(image, (224,224))\n    # print(image.shape)\n    X_train.append(image)\n  else:\n    pass","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:40:42.927397Z","iopub.execute_input":"2024-06-04T12:40:42.927828Z","iopub.status.idle":"2024-06-04T12:40:57.160425Z","shell.execute_reply.started":"2024-06-04T12:40:42.927782Z","shell.execute_reply":"2024-06-04T12:40:57.159539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train=np.array(X_train)\nX_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:41:05.072708Z","iopub.execute_input":"2024-06-04T12:41:05.073609Z","iopub.status.idle":"2024-06-04T12:41:05.138479Z","shell.execute_reply.started":"2024-06-04T12:41:05.073578Z","shell.execute_reply":"2024-06-04T12:41:05.137550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('./dgt301_turn2-012_loss-0.0300_val_loss-0.2798.weights.h5')","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:57:10.321885Z","iopub.execute_input":"2024-06-04T12:57:10.322241Z","iopub.status.idle":"2024-06-04T12:57:11.619697Z","shell.execute_reply.started":"2024-06-04T12:57:10.322216Z","shell.execute_reply":"2024-06-04T12:57:11.618932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32 \n\nstacked = 0\n#Y=np.append(Y_1, Y_2, axis=0)\n\nfor i in range(len(X_train)):\n    if i % batch_size == 0:\n        if stacked != 0: # First batch\n            if stacked == 1:\n                y = model(X_train[i-32:i])\n                Y = np.array(y)\n            else:\n                y = model(X_train[i-32:i])\n                Y = np.append(Y,y,axis=0)\n            print(i-32,i,Y.shape)       \n        stacked += 1\nrest_output = model(X_train[(stacked-1)*32:])\n\nY = np.append(Y,rest_output,axis=0)\n\nprint(Y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:52:57.776266Z","iopub.execute_input":"2024-06-04T12:52:57.776734Z","iopub.status.idle":"2024-06-04T12:53:22.463230Z","shell.execute_reply.started":"2024-06-04T12:52:57.776699Z","shell.execute_reply":"2024-06-04T12:53:22.462021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = np.array(Y)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:53:26.114307Z","iopub.execute_input":"2024-06-04T12:53:26.114658Z","iopub.status.idle":"2024-06-04T12:53:26.120078Z","shell.execute_reply.started":"2024-06-04T12:53:26.114633Z","shell.execute_reply":"2024-06-04T12:53:26.118856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\nwith open('submission_B4_bestloss_0.98.csv', mode='w') as employee_file:\n    employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n    employee_writer.writerow(['Id'])\n    j = 0\n    cnt = 0\n    for i in data_array:\n        image = cv2.imread(folder_path + '/' + str(i[0]) + '.jpg')\n        cnt += 1\n        if image is not None:\n            if Y[j] > 0.98:\n                employee_writer.writerow([i[0]])\n            else:\n                pass\n            j += 1\n        else:\n            pass\n    print(cnt)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T13:37:00.073506Z","iopub.execute_input":"2024-06-04T13:37:00.073901Z","iopub.status.idle":"2024-06-04T13:37:08.045437Z","shell.execute_reply.started":"2024-06-04T13:37:00.073872Z","shell.execute_reply":"2024-06-04T13:37:08.044413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport csv\n\n# Đường dẫn đến tệp nhãn\nlabel_file = \"/kaggle/input/2024-sum-dpl-302-m/test.csv\"\n\n# Thư mục chứa hình ảnh kiểm tra\nfolder_path = \"/kaggle/input/2024-sum-dpl-302-m/testset_images/testset_images\"\n\n# Đọc dữ liệu từ tệp nhãn\ndata = pd.read_csv(label_file)\ndata_array = data.values\n\n# Đọc và xử lý hình ảnh từ thư mục\nX_test = []\nfor i in data_array:\n    image = cv2.imread(folder_path + '/' + str(i[0]) + '.jpg')\n    if image is not None:\n        image = cv2.resize(image, (224, 224))\n        X_test.append(image)\n\n# Chuyển danh sách hình ảnh thành mảng numpy\nX_test = np.array(X_test)\n\n# Tải trọng số của mô hình đã được huấn luyện trước\nmodel.load_weights('./dgt301_turn2-012_loss-0.0300_val_loss-0.2798.weights.h5')\n\n# Dự đoán\nY_pred = model.predict(X_test)\n\n# Ghi kết quả vào tệp CSV\nwith open('test.csv', mode='w') as file:\n    writer = csv.writer(file)\n    writer.writerow(['image_id', 'label'])  # Viết tiêu đề cột\n    for i, prediction in enumerate(Y_pred):\n        if prediction > 0.99:  # Kiểm tra ngưỡng\n            writer.writerow([data_array[i][0], 1])  # Ghi Id của hình ảnh và label dự đoán vào tệp\n        else:\n            writer.writerow([data_array[i][0], 0])  # Ghi Id của hình ảnh và label dự đoán vào tệp\n","metadata":{"execution":{"iopub.status.busy":"2024-06-04T12:57:54.153923Z","iopub.execute_input":"2024-06-04T12:57:54.154577Z","iopub.status.idle":"2024-06-04T12:58:10.108043Z","shell.execute_reply.started":"2024-06-04T12:57:54.154547Z","shell.execute_reply":"2024-06-04T12:58:10.107058Z"},"trusted":true},"execution_count":null,"outputs":[]}]}